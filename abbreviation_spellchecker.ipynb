{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates a spell-checking algorithm, that helps restoring abbreviations based on the Lord of the Rings vocabulary!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sampled from: [abbreviation_spellchecker](https://github.com/avidale/weirdMath/blob/master/nlp/abbreviation_spellchecker_Frodo.ipynb)\n",
    "\n",
    "The problem is how to recover whole words from abbreviations, like these\n",
    "```\n",
    "wtrbtl = water bottle\n",
    "bwlingbl = bowling ball\n",
    "bsktball = basketball\n",
    "```\n",
    "without comprehensive dictionary of whole words.\n",
    "\n",
    "I use [noisy channel approach](http://web.stanford.edu/~jurafsky/slp3/5.pdf), which considers each abbreviation as a distorted version of the original phrase. \n",
    "\n",
    "To recover the original phrase, I need to answer to questions: which distortions are likely, and which phrases are likely.\n",
    "\n",
    "By Bayes theorem, $p(phrase|abbreviation) \\sim p(phrase) p(abbreviation|phrase) = p(phrase) \\sum p(distortion|phrase) $, where $distortion$, applied to the original $phrase$, generates the observable phrase - $abbreviation$. \n",
    "\n",
    "Both right-hand sided conditional probabilities may be evaluated by NLP models. I will use the simplest class of models - character based n-grams.\n",
    "\n",
    "With these models, I will perform approximate (beam) search for the most likely originall phrases, letter-by-letter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I create the language model. \n",
    "\n",
    "It merges weighted frequencies of n, n-1, ..., 0 gram-models to give the most stable estimiate of distribution of next each letter in a phrase based on previous letters. It is one of the [vanilla n-gram models](https://en.wikipedia.org/wiki/N-gram)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class LanguageNgramModel:\n",
    "    \"\"\" Remember and predict which letters usually follows which. \"\"\"\n",
    "    def __init__(self, order=1, smoothing=1.0, recursive=0.001):\n",
    "        self.order = order\n",
    "        self.smoothing = smoothing\n",
    "        self.recursive = recursive\n",
    "    \n",
    "    def fit(self, corpus):\n",
    "        \"\"\" Estimate all counts on a text \"\"\"\n",
    "        self.counter_ = defaultdict(lambda: Counter())\n",
    "        self.unigrams_ = Counter()\n",
    "        self.vocabulary_ = set()\n",
    "        for i, token in enumerate(corpus[self.order:]):\n",
    "            context = corpus[i:(i+self.order)]\n",
    "            self.counter_[context][token] += 1\n",
    "            self.unigrams_[token] +=1\n",
    "            self.vocabulary_.add(token)\n",
    "        self.vocabulary_ = sorted(list(self.vocabulary_))\n",
    "        if self.recursive > 0 and self.order > 0:\n",
    "            self.child_ = LanguageNgramModel(self.order-1, self.smoothing, self.recursive)\n",
    "            self.child_.fit(corpus)\n",
    "            \n",
    "    def get_counts(self, context):\n",
    "        \"\"\" Get smoothed count of each letter appearing after context \"\"\"\n",
    "        if self.order:\n",
    "            local = context[-self.order:]\n",
    "        else:\n",
    "            local = ''\n",
    "        freq_dict = self.counter_[local]\n",
    "        freq = pd.Series(index=self.vocabulary_)\n",
    "        for i, token in enumerate(self.vocabulary_):\n",
    "            freq[token] = freq_dict[token] + self.smoothing\n",
    "        if self.recursive > 0 and self.order > 0:\n",
    "            child_freq = self.child_.get_counts(context) * self.recursive\n",
    "            freq += child_freq\n",
    "        return freq\n",
    "    \n",
    "    def predict_proba(self, context):\n",
    "        \"\"\" Get smoothed probability of each letter appearing after context \"\"\"\n",
    "        counts = self.get_counts(context)\n",
    "        return counts / counts.sum()\n",
    "    \n",
    "    def single_log_proba(self, context, continuation):\n",
    "        \"\"\" Estimate log-probability that context is followed by continuation \"\"\"\n",
    "        result = 0.0\n",
    "        for token in continuation:\n",
    "            result += np.log(self.predict_proba(context)[token])\n",
    "            context += token\n",
    "        return result\n",
    "    \n",
    "    def single_proba(self, context, continuation):\n",
    "        \"\"\" Estimate probability that context is followed by continuation \"\"\"\n",
    "        return np.exp(self.single_log_proba(context, continuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I create the missing letter model. It is also based on n-grams, but it estimates probability of each letter being missed based on the previous letters.\n",
    "\n",
    "This model would be trained on a much smaller dataset, labeled manually. And it will itself be small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MissingLetterModel:\n",
    "    \"\"\" Remember and predict which letters are usually missing. \"\"\"\n",
    "    def __init__(self, order=0, smoothing_missed=0.3, smoothing_total=1.0):\n",
    "        self.order = order\n",
    "        self.smoothing_missed = smoothing_missed\n",
    "        self.smoothing_total = smoothing_total\n",
    "    def fit(self, sentence_pairs):\n",
    "        self.missed_counter_ = defaultdict(lambda: Counter())\n",
    "        self.total_counter_ = defaultdict(lambda: Counter())\n",
    "        for (original, observed) in sentence_pairs:\n",
    "            for i, (original_letter, observed_letter) in enumerate(zip(original[self.order:], observed[self.order:])):\n",
    "                context = original[i:(i+self.order)]\n",
    "                if observed_letter == '-':\n",
    "                    self.missed_counter_[context][original_letter] += 1\n",
    "                self.total_counter_[context][original_letter] += 1 \n",
    "    def predict_proba(self, context, last_letter):\n",
    "        \"\"\" Estimate probability that last_letter after context is missed \"\"\"\n",
    "        if self.order:\n",
    "            local = context[-self.order:]\n",
    "        else:\n",
    "            local = ''\n",
    "        missed_freq = self.missed_counter_[local][last_letter] + self.smoothing_missed\n",
    "        total_freq = self.total_counter_[local][last_letter] + self.smoothing_total\n",
    "        return missed_freq / total_freq\n",
    "    \n",
    "    def single_log_proba(self, context, continuation, actual=None):\n",
    "        \"\"\" Estimate log-probability of continuaton being distorted to actual after context. \n",
    "        If actual is None, assume no distortion\n",
    "        \"\"\"\n",
    "        if not actual:\n",
    "            actual = continuation\n",
    "        result = 0.0\n",
    "        for orig_token, act_token in zip(continuation, actual):\n",
    "            pp = self.predict_proba(context, orig_token)\n",
    "            if act_token == '-':\n",
    "                pp = 1 - pp\n",
    "            result += np.log(pp)\n",
    "            context += orig_token\n",
    "        return result\n",
    "    \n",
    "    def single_proba(self, context, continuation, actual=None):\n",
    "        \"\"\" Estimate probability of continuaton being distorted to actual after context. \n",
    "        If actual is None, assume no distortion\n",
    "        \"\"\"\n",
    "        return np.exp(self.single_log_proba(context, continuation, actual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start wiht a simple example. I train my language model on a single word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     0.181777\n",
       "a    0.091297\n",
       "b    0.272529\n",
       "c    0.181686\n",
       "d    0.181686\n",
       "r    0.091025\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_model = LanguageNgramModel(1)\n",
    "lang_model.fit(' abracadabra ')\n",
    "lang_model.predict_proba(' bra')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I train my distortion model on a single (original word, distortion) pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7166666666666667, 0.09999999999999999)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missed_model = MissingLetterModel(0)\n",
    "missed_model.fit([('abracadabra', 'abr-c-d-br-')]) \n",
    "missed_model.predict_proba('abr', 'a'), missed_model.predict_proba('abr', 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0020305555555555532"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missed_model.single_proba('', 'abra', 'abr-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Approach to the search**\n",
    "\n",
    "For simplicity, we let each query (distorted phrase) start and end with whitespaces (beginning and end of phrases).\n",
    "\n",
    "We will have an infinite tree which we hant to explore for the best (most probable) path from the root to a leaf.\n",
    "\n",
    "The root is the beginning of the phease. Each edge is an additional letter, and it can be missed or intact. Each edge is assigned probability conditional on several previous edges. Thus probability of each path is the product of probabilities of its edges (or sum of log-probabilities).\n",
    "\n",
    "A node is declared as a leaf, if its path, with the missed letters dropped, equals the query. \n",
    "\n",
    "**Search algorithm**\n",
    "\n",
    "This tree is possibly infinite, but we need only the good leaves. So we do a kind of beam search: \n",
    " * at each node, estimate log-probability of its ancestor leaves as $optimism \\times default$, where $optimism$ is a user-provided coefficient, and $default$ is the unconditional probability of the corresponding (unprocessed) phrase suffix at this node. \n",
    " * look only at the nodes with estimate of log-likelihood no less than best current log-likelihood minus $freedom$.\n",
    " \n",
    "So, the lower $optimism$ and the higher $freedom$, the slower the search will be, and the more paths will be explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from heapq import heappush, heappop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function generates children for each node and estimates likelihood of their ancestor leaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6.929663174828117, '  ', 'brac ', ' ', 3.7800651217336947), (5.042879645338754, ' a', 'brac ', 'a', 3.4572571306016755), (8.09487194753453, ' b', 'brac ', 'b', 3.846661605771999), (7.623807861705187, ' c', 'brac ', 'c', 3.7800651217336947), (7.623807861705187, ' d', 'brac ', 'd', 3.7800651217336947), (8.09487194753453, ' r', 'brac ', 'r', 3.846661605771999), (4.858238261775765, ' b', 'rac ', '', 2.8072524973494524)]\n"
     ]
    }
   ],
   "source": [
    "def generate_options(prefix_proba, prefix, suffix, lang_model, missed_model, optimism=0.5, cache=None):\n",
    "    options = []\n",
    "    for letter in lang_model.vocabulary_ + ['']:\n",
    "        if letter:  # assume a missing letter\n",
    "            next_letter = letter\n",
    "            new_suffix = suffix\n",
    "            new_prefix = prefix + next_letter\n",
    "            proba_missing_state = - np.log(missed_model.predict_proba(prefix, letter))\n",
    "        else:  # assume no missing letter\n",
    "            next_letter = suffix[0]\n",
    "            new_suffix = suffix[1:]\n",
    "            new_prefix = prefix + next_letter\n",
    "            proba_missing_state = - np.log((1 - missed_model.predict_proba(prefix, next_letter)))\n",
    "        proba_next_letter = - np.log(lang_model.single_proba(prefix, next_letter))\n",
    "        if cache:\n",
    "            proba_suffix = cache[len(new_suffix)] * optimism\n",
    "        else:\n",
    "            proba_suffix = - np.log(lang_model.single_proba(new_prefix, new_suffix)) * optimism\n",
    "        proba = prefix_proba + proba_next_letter + proba_missing_state + proba_suffix\n",
    "        options.append((proba, new_prefix, new_suffix, letter, proba_suffix))\n",
    "    return options\n",
    "print(generate_options(0, ' ', 'brac ', lang_model, missed_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function explores the graph on noisy channel in the best-first manner, until it runs out of attempts or out of optimistic nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noisy_channel(word, lang_model, missed_model, freedom=1.0, max_attempts=1000, optimism=0.1, verbose=True):\n",
    "    query = word + ' '\n",
    "    prefix = ' '\n",
    "    prefix_proba = 0.0\n",
    "    suffix = query\n",
    "    full_origin_logprob = -lang_model.single_log_proba(prefix, query)\n",
    "    no_missing_logprob = -missed_model.single_log_proba(prefix, query)\n",
    "    best_logprob = full_origin_logprob + no_missing_logprob\n",
    "    # add empty beginning to the heap\n",
    "    heap = [(best_logprob * optimism, prefix, suffix, '', best_logprob * optimism)]\n",
    "    # add the default option (no missing letters) to candidates\n",
    "    candidates = [(best_logprob, prefix + query, '', None, 0.0)]\n",
    "    if verbose:\n",
    "        # todo: include distortion probability\n",
    "        print('baseline score is', best_logprob)\n",
    "    # prepare cache for suffixes (the slowest operation)\n",
    "    cache = {}\n",
    "    for i in range(len(query)+1):\n",
    "        future_suffix = query[:i]\n",
    "        cache[len(future_suffix)] = -lang_model.single_log_proba('', future_suffix) # rough approximation\n",
    "        cache[len(future_suffix)] += -missed_model.single_log_proba('', future_suffix) # at least add missingness\n",
    "    \n",
    "    for i in range(max_attempts):\n",
    "        if not heap:\n",
    "            break\n",
    "        next_best = heappop(heap)\n",
    "        if verbose:\n",
    "            print(next_best)\n",
    "        if next_best[2] == '':  # it is a leaf\n",
    "            # this is the best leaf as far, add it to candidates\n",
    "            if next_best[0] <= best_logprob + freedom:\n",
    "                candidates.append(next_best)\n",
    "                # update the best likelihood\n",
    "                if next_best[0] < best_logprob:\n",
    "                    best_logprob = next_best[0]\n",
    "        else: # it is not a leaf - generate more options\n",
    "            prefix_proba = next_best[0] - next_best[4] # all proba estimate minus suffix\n",
    "            prefix = next_best[1]\n",
    "            suffix = next_best[2]\n",
    "            new_options = generate_options(prefix_proba, prefix, suffix, lang_model, missed_model, optimism, cache)\n",
    "            # add only the solution potentioally no worse than the best + freedom\n",
    "            for new_option in new_options: \n",
    "                if new_option[0] < best_logprob + freedom:\n",
    "                    heappush(heap, new_option)\n",
    "    if verbose:\n",
    "        print('heap size is', len(heap), 'after', i, 'iterations')\n",
    "    result = {}\n",
    "    for candidate in candidates:\n",
    "        if candidate[0] <= best_logprob + freedom:\n",
    "            result[candidate[1][1:-1]] = candidate[0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply this function to the abbreviation 'brc' and look for suggested options with scores (the lower the better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline score is 14.659531132722798\n",
      "(7.329765566361399, ' ', 'brc ', '', 7.329765566361399)\n",
      "(7.729102491649175, ' b', 'rc ', '', 5.6781167272228625)\n",
      "(6.82819709010665, ' br', 'c ', '', 3.689648873198813)\n",
      "(7.4281382278577714, ' brc', ' ', '', 2.0472553582899407)\n",
      "(7.68318306227505, ' brc ', '', '', -0.0)\n",
      "(8.142544971129297, ' bra', 'c ', 'a', 3.689648873198813)\n",
      "(8.36814476033081, ' brac', ' ', '', 2.0472553582899407)\n",
      "(8.623189594748087, ' brac ', '', '', -0.0)\n",
      "(8.838538268507152, ' a', 'brc ', 'a', 7.252915753770074)\n",
      "(8.669109024122214, ' ab', 'rc ', '', 5.6781167272228625)\n",
      "(7.768203622579689, ' abr', 'c ', '', 3.689648873198813)\n",
      "(8.36814476033081, ' abrc', ' ', '', 2.0472553582899407)\n",
      "(8.623189594748087, ' abrc ', '', '', -0.0)\n",
      "(9.013760742594851, ' brca', ' ', 'a', 2.0472553582899407)\n",
      "(9.028155327065601, ' brca ', '', '', -0.0)\n",
      "(9.082551503602335, ' abra', 'c ', 'a', 3.689648873198813)\n",
      "(9.30815129280385, ' abrac', ' ', '', 2.0472553582899407)\n",
      "(9.563196127221126, ' abrac ', '', '', -0.0)\n",
      "(10.11098811127768, ' br ', 'c ', ' ', 3.689648873198813)\n",
      "(10.138078592325058, ' ba', 'rc ', 'a', 5.6781167272228625)\n",
      "(10.402513806864496, '  ', 'brc ', ' ', 7.252915753770074)\n",
      "(10.577736280952195, ' brc ', ' ', ' ', 2.0472553582899407)\n",
      "(10.80513279815475, ' brc', 'c ', 'c', 3.689648873198813)\n",
      "(10.80513279815475, ' brd', 'c ', 'd', 3.689648873198813)\n",
      "(11.011893512820205, ' b ', 'rc ', ' ', 5.6781167272228625)\n",
      "(11.013889521466918, ' br', 'rc ', 'r', 5.6781167272228625)\n",
      "(11.096658493741566, ' c', 'brc ', 'c', 7.252915753770074)\n",
      "(11.096658493741566, ' d', 'brc ', 'd', 7.252915753770074)\n",
      "(11.209600399945788, ' brb', 'c ', 'b', 3.689648873198813)\n",
      "(11.209600399945788, ' brr', 'c ', 'r', 3.689648873198813)\n",
      "(11.271880967829265, ' brcc', ' ', 'c', 2.0472553582899407)\n",
      "(11.271880967829265, ' brcd', ' ', 'd', 2.0472553582899407)\n",
      "(11.501126095532605, ' b', 'brc ', 'b', 7.252915753770074)\n",
      "(11.501126095532605, ' r', 'brc ', 'r', 7.252915753770074)\n",
      "(11.676348569620306, ' brcb', ' ', 'b', 2.0472553582899407)\n",
      "(11.676348569620306, ' brcr', ' ', 'r', 2.0472553582899407)\n",
      "(11.706038199697275, ' bc', 'rc ', 'c', 5.6781167272228625)\n",
      "(11.706038199697275, ' bd', 'rc ', 'd', 5.6781167272228625)\n",
      "(12.110505801488314, ' bb', 'rc ', 'b', 5.6781167272228625)\n",
      "heap size is 0 after 39 iterations\n",
      "{'brc': 7.68318306227505, 'brac': 8.623189594748087, 'abrc': 8.623189594748087, 'brca': 9.028155327065601, 'abrac': 9.563196127221126}\n"
     ]
    }
   ],
   "source": [
    "result = noisy_channel('brc', lang_model, missed_model, freedom=2.0, optimism=0.5, verbose=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model on corpus "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train a good language model on a large corpus - Lord of the Rings!\n",
    "\n",
    "To start, drop all characters except spaces and letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annual report  petronas chemicals group berhad wwwpetronaschemicalscom shareholder value environment\n"
     ]
    }
   ],
   "source": [
    "with open('corpus\\Corpus.txt', encoding = 'utf-8') as f:\n",
    "    text = f.read()\n",
    "import re\n",
    "text2 = re.sub(r'[^a-z ]+', '', text.lower().replace('\\n', ' '))\n",
    "print(text2[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' abcdefghijklmnopqrstuvwxyz'\n"
     ]
    }
   ],
   "source": [
    "all_letters = ''.join(list(sorted(list(set(text2)))))\n",
    "print(repr(all_letters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare a small training corpus for the missing-word model, that shows that letters 'aeiouy' are missed more frequently "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "missing_set = [\n",
    "] + [(all_letters, '-' * len(all_letters))] * 3 + [(all_letters, all_letters)] * 10 + [('aeiouy', '------')] * 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the best model order by comparing log likelihoods on the end of the book ('test set'). \n",
    "\n",
    "The longer memory, the better. But after order 4 the gain is not so large, so we stop here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -14306.373304419854\n",
      "1 -12687.401358449732\n",
      "2 -10331.869983054516\n",
      "3 -7416.3294112879385\n",
      "4 -6018.242023578867\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    tmp = LanguageNgramModel(i, 1.0, 0.001)\n",
    "    tmp.fit(text2[0:-5000])\n",
    "    print(i, tmp.single_log_proba(' ', text2[-5000:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the 5-gram language model and 1-gram missing letter model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "big_lang_m = LanguageNgramModel(4, 0.001, 0.01)\n",
    "big_lang_m.fit(text2)\n",
    "big_err_m = MissingLetterModel(0, 0.1)\n",
    "big_err_m.fit(missing_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply our algorithm to different abbreviations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'petrol': 11.94817530941005, 'petronas': 10.679361014950668}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_channel('ptro', big_lang_m, big_err_m, max_attempts=10000, optimism=0.9, freedom=3.0, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'enty key': 32.12997944715083}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_channel('enty key', big_lang_m, big_err_m, max_attempts=10000, optimism=0.9, freedom=3.0, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entify key': 25.13389562698879}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_channel('enty key', big_lang_m, big_err_m, max_attempts=10000, optimism=0.5, freedom=0.5, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entry key': 28.68073178382995}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_channel('enty key', big_lang_m, big_err_m, max_attempts=10000, optimism=0.7, freedom=0.1, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, with the relevant training corpus the sports words would be recognized."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
